# Training configuration
vocab_path: dataset/wikitext/vocab.json
finetune_data_path: dataset/snli
device: cuda
lr: 0.001

Train:
  batch_size: 4048
  shuffle: True
  num_workers: 8
  pin_memory: False
  max_len: 64
  resume: False
  epochs: 200

Eval:
  batch_size: 2048
  shuffle: True
  num_workers: 8
  pin_memory: False

# Model configuration
num_hiddens: 128
ffn_num_hiddens: 256
num_heads: 2
num_blks: 2
dropout: 0.2
max_len: 64

# Debugging
log_file: logs/pretrain.log
tensorboard: debugs/tensorboard

single_classification: 
  best_ckpt_path: weights/finetune_single_cls/best.pth
  last_ckpt_path: weights/finetune_single_cls/last.pth

pretrain_checkpoints:
  best_ckpt_path: weights/pretrain/best.pth
  last_ckpt_path: weights/pretrain/last.pth
  