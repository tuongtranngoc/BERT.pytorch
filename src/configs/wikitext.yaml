# Training configuration
pretrain_data_path: dataset/wikitext
vocab_path: dataset/wikitext/vocab.json
finetune_data_path: dataset/snli
batch_size: 2048
shuffle: True
num_workers: 8
pin_memory: False
max_len: 64
lr: 0.01
resume: False
epochs: 200
device: cuda

# Model configuration
num_hiddens: 128
ffn_num_hiddens: 256
num_heads: 2
num_blks: 2
dropout: 0.2

# Debugging
log_file: logs/pretrain.log
tensorboard: debugs/tensorboard

pretrain_checkpoints:
  best_ckpt_path: weights/pretrain/best.pth
  last_ckpt_path: weights/pretrain/last.pth

finetune_checkpoints:
  single_classification: 
    best_ckpt_path: weights/finetune_single_cls/best.pth
    last_ckpt_path: weights/finetune_single_cls/last.pth
  